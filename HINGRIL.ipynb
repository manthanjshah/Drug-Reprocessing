{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manthanjshah/Drug-Reprocessing/blob/main/HINGRIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6kyphTyQblz",
        "outputId": "8c4ba95c-481b-448e-ba55-6f3efc02d0ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      drugName                                             SMILES\n",
            "0     clobazam       CN1C(=O)CC(=O)N(C2=C1C=CC(=C2)Cl)C3=CC=CC=C3\n",
            "1  vinorelbine  CCC1=CC2CC(C3=C(CN(C2)C1)C4=CC=CC=C4N3)(C5=C(C...\n",
            "2   carvedilol  COC1=CC=CC=C1OCCNCC(COC2=CC=CC3=C2C4=CC=CC=C4N3)O\n",
            "3   benazepril  CCOC(=O)C(CCC1=CC=CC=C1)NC2CCC3=CC=CC=C3N(C2=O...\n",
            "4  leflunomide            CC1=C(C=NO1)C(=O)NC2=CC=C(C=C2)C(F)(F)F\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drugs_with_smiles.csv')\n",
        "\n",
        "# Display the first few rows to check column names and data\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Code"
      ],
      "metadata": {
        "id": "U8hLEbWM4oYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import scipy.sparse as sp\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Partition function\n",
        "def partition(ls, size):\n",
        "    return [ls[i:i + size] for i in range(0, len(ls), size)]\n",
        "\n",
        "# Generate Negative Samples\n",
        "def NegativeGenerate(DrugDisease, AllDrug, AllDisease):\n",
        "    NegativeSample = []\n",
        "    counterN = 0\n",
        "    while counterN < len(DrugDisease):\n",
        "        counterR = random.randint(0, len(AllDrug) - 1)\n",
        "        counterD = random.randint(0, len(AllDisease) - 1)\n",
        "        DiseaseAndRnaPair = [AllDrug[counterR], AllDisease[counterD]]\n",
        "        if DiseaseAndRnaPair in DrugDisease or DiseaseAndRnaPair in NegativeSample:\n",
        "            continue\n",
        "        NegativeSample.append(DiseaseAndRnaPair)\n",
        "        counterN += 1\n",
        "    return NegativeSample\n",
        "\n",
        "# Main function\n",
        "def main(options):\n",
        "    if options.dataset == 1:\n",
        "        dataset = 'B-Dataset'\n",
        "    else:\n",
        "        dataset = 'F-Dataset'\n",
        "    print('fold', options.fold_num)\n",
        "    print(dataset)\n",
        "\n",
        "    # Load data\n",
        "    DrDiNum18416 = pd.read_csv('/content/DrDiNum.csv', header=None)\n",
        "    DrPrNum3243 = pd.read_csv('/content/DrPrNum.csv', header=None)\n",
        "    DiPrNum71840 = pd.read_csv('/content/DiPrNum.csv', header=None)\n",
        "\n",
        "    # Shuffle and partition the data\n",
        "    RandomList = random.sample(range(0, len(DrDiNum18416)), len(DrDiNum18416))\n",
        "    print('len(RandomList)', len(RandomList))\n",
        "    NewRandomList = partition(RandomList, math.ceil(len(RandomList) / options.fold_num))\n",
        "    print('len(NewRandomList[0])', len(NewRandomList[0]))\n",
        "    NewRandomList = pd.DataFrame(NewRandomList).fillna(0).astype(int)\n",
        "    NewRandomList.to_csv('/content/NewRandomList.csv', header=None, index=False)\n",
        "    del NewRandomList, RandomList\n",
        "\n",
        "    # Cross-validation split\n",
        "    Nindex = pd.read_csv('/content/NewRandomList.csv', header=None)\n",
        "    for i in range(len(Nindex)):\n",
        "        kk = [j for j in range(options.fold_num) if j != i]\n",
        "        index = np.hstack([np.array(Nindex)[k] for k in kk])\n",
        "\n",
        "        # Ensure that indices exist before trying to access them\n",
        "        valid_index = [idx for idx in index if idx in DrDiNum18416.index]\n",
        "\n",
        "        DTIs_train = pd.DataFrame(np.array(DrDiNum18416)[valid_index])\n",
        "        DTIs_train.to_csv(f'/content/DrDiIs_train{i}.csv', header=None, index=False)\n",
        "\n",
        "        test_indices = np.array(Nindex)[i].tolist()\n",
        "        valid_test_indices = [idx for idx in test_indices if idx in DrDiNum18416.index]\n",
        "\n",
        "        DTIs_test = pd.DataFrame(np.array(DrDiNum18416)[valid_test_indices])\n",
        "        DTIs_test.to_csv(f'/content/DrDiIs_test{i}.csv', header=None, index=False)\n",
        "        print(i)\n",
        "    del Nindex, index, DTIs_train, DTIs_test\n",
        "\n",
        "    # Combine data\n",
        "    DTIs_train = pd.concat([DrDiNum18416, DrPrNum3243, DiPrNum71840]).sample(frac=1.0)\n",
        "    DTIs_train.to_csv('/content/AllDrDiIs_train.txt', sep='\\t', header=None, index=False)\n",
        "\n",
        "    # Generate negative samples\n",
        "    Dr = pd.read_csv('/content/drugName.csv', header=0, names=['id', 'name'])\n",
        "    Pr = pd.read_csv('/content/diseaseName.csv', header=0, names=['id', 'name'])\n",
        "    NegativeSample = NegativeGenerate(DrDiNum18416.values.tolist(), Dr['id'].values.tolist(), Pr['id'].values.tolist())\n",
        "    NegativeSample = pd.DataFrame(NegativeSample)\n",
        "    NegativeSample.to_csv('/content/NegativeSample.csv', header=None, index=False)\n",
        "\n",
        "    # Load additional data\n",
        "    Negative = pd.read_csv('/content/NegativeSample.csv', header=None)\n",
        "    Nindex = pd.read_csv('/content/NewRandomList.csv', header=None)\n",
        "    Attribute = pd.read_csv('/content/AllNodeAttribute.csv', header=None, index_col=0).iloc[:, 1:]\n",
        "    Embedding = pd.read_csv('/content/AllEmbedding_DeepWalk.txt', sep=' ', header=None, skiprows=1)\n",
        "    Embedding = Embedding.sort_values(0, ascending=True).set_index([0])\n",
        "    print(Embedding)\n",
        "\n",
        "    # Prepare training and testing data for each fold\n",
        "    for i in range(options.fold_num):\n",
        "        train_data = pd.read_csv(f'/content/DrDiIs_train{i}.csv', header=None)\n",
        "        train_data[2] = 1\n",
        "        kk = [j for j in range(options.fold_num) if j != i]\n",
        "        index = np.hstack([np.array(Nindex)[k] for k in kk])\n",
        "        result = pd.concat([train_data, pd.DataFrame(np.array(Negative)[index])])\n",
        "        labels_train = result[2]\n",
        "        data_train_feature = pd.concat([\n",
        "            pd.concat([Attribute.loc[result[0].values.tolist()], Embedding.loc[result[0].values.tolist()]], axis=1).reset_index(drop=True),\n",
        "            pd.concat([Attribute.loc[result[1].values.tolist()], Embedding.loc[result[1].values.tolist()]], axis=1).reset_index(drop=True)\n",
        "        ], axis=1)\n",
        "\n",
        "        # Replace NaN values in labels_train with 0\n",
        "        labels_train.fillna(0, inplace=True)\n",
        "\n",
        "        globals()[f'data_train{i}'] = data_train_feature.values.tolist()\n",
        "        globals()[f'labels_train{i}'] = labels_train\n",
        "        print(len(labels_train))\n",
        "        del result, data_train_feature\n",
        "\n",
        "        test_data = pd.read_csv(f'/content/DrDiIs_test{i}.csv', header=None)\n",
        "        test_data[2] = 1\n",
        "        result = pd.concat([test_data, pd.DataFrame(np.array(Negative)[np.array(Nindex)[i]])])\n",
        "\n",
        "        # Replace NaN values in labels_test with 0\n",
        "        labels_test = result[2]\n",
        "        labels_test.fillna(0, inplace=True)\n",
        "\n",
        "        data_test_feature = pd.concat([\n",
        "            pd.concat([Attribute.loc[result[0].values.tolist()], Embedding.loc[result[0].values.tolist()]], axis=1).reset_index(drop=True),\n",
        "            pd.concat([Attribute.loc[result[1].values.tolist()], Embedding.loc[result[1].values.tolist()]], axis=1).reset_index(drop=True)\n",
        "        ], axis=1)\n",
        "        globals()[f'data_test{i}'] = data_test_feature.values.tolist()\n",
        "        globals()[f'labels_test{i}'] = labels_test\n",
        "        print(len(labels_test))\n",
        "        del test_data, labels_test, result, data_test_feature\n",
        "        print(i)\n",
        "\n",
        "    # Aggregate data for training and testing\n",
        "    data_train = [globals()[f'data_train{i}'] for i in range(options.fold_num)]\n",
        "    data_test = [globals()[f'data_test{i}'] for i in range(options.fold_num)]\n",
        "    labels_train = [globals()[f'labels_train{i}'] for i in range(options.fold_num)]\n",
        "    labels_test = [globals()[f'labels_test{i}'] for i in range(options.fold_num)]\n",
        "\n",
        "    # Cross-validation and ROC AUC computation\n",
        "    print(f\"{options.fold_num}-CV\")\n",
        "    tprs = []\n",
        "    aucs = []\n",
        "    mean_fpr = np.linspace(0, 1, 1000)\n",
        "    AllResult = []\n",
        "\n",
        "    for i in range(options.fold_num):\n",
        "        X_train, X_test = data_train[i], data_test[i]\n",
        "        Y_train, Y_test = np.array(labels_train[i]), np.array(labels_test[i])\n",
        "\n",
        "        # Replace NaN values with 0 in Y_train and Y_test\n",
        "        Y_train[np.isnan(Y_train)] = 0\n",
        "        Y_test[np.isnan(Y_test)] = 0\n",
        "\n",
        "        best_RandomF = RandomForestClassifier(n_estimators=options.tree_number)\n",
        "        best_RandomF.fit(X_train, Y_train)\n",
        "\n",
        "        # Predict probabilities and handle NaN values in y_score_RandomF\n",
        "        y_score_RandomF = best_RandomF.predict_proba(X_test)\n",
        "        y_score_RandomF[np.isnan(y_score_RandomF)] = 0\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(Y_test, y_score_RandomF[:, 1])\n",
        "        tprs.append(np.interp(mean_fpr, fpr, tpr))  # Changed here to np.interp\n",
        "        tprs[-1][0] = 0.0\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        aucs.append(roc_auc)\n",
        "        print('ROC fold %d (AUC=%0.4f)' % (i, roc_auc))\n",
        "\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    print('Mean ROC (AUC=%0.4f)' % mean_auc)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import optparse\n",
        "    import sys\n",
        "\n",
        "    # Check if running in a Jupyter notebook or parse command-line arguments\n",
        "    if 'ipykernel' in sys.modules or any('jupyter' in arg for arg in sys.argv):\n",
        "        class Options:\n",
        "            def __init__(self, dataset, fold_num, tree_number):\n",
        "                self.dataset = dataset\n",
        "                self.fold_num = fold_num\n",
        "                self.tree_number = tree_number\n",
        "\n",
        "        options = Options(dataset=1, fold_num=5, tree_number=100)  # Example options, customize as needed\n",
        "    else:\n",
        "        optparser = optparse.OptionParser()\n",
        "        optparser.add_option('-d', '--dataset', dest='dataset', type='int', default=1)\n",
        "        optparser.add_option('-f', '--fold_num', dest='fold_num', type='int', default=5)\n",
        "        optparser.add_option('-t', '--tree_number', dest='tree_number', type='int', default=100)\n",
        "        (options, args) = optparser.parse_args()\n",
        "\n",
        "    main(options)\n"
      ],
      "metadata": {
        "id": "0JHxSTzAw98S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d898f11c-5e2a-41a7-83a9-49e3ee1dc108",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 5\n",
            "B-Dataset\n",
            "len(RandomList) 18416\n",
            "len(NewRandomList[0]) 3684\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "            1         2         3         4         5         6         7   \\\n",
            "0                                                                            \n",
            "0     0.036320  0.114171 -0.111191 -0.306078 -0.150277 -0.242947  0.091169   \n",
            "1     0.305414  0.095406  0.015667 -0.047628 -0.128059 -0.174679  0.038152   \n",
            "2    -0.256181  0.181465  0.079849 -0.457883 -0.287999 -0.031660  0.076192   \n",
            "3     0.001911  0.204960  0.024538 -0.404577 -0.621718 -0.363406  0.318680   \n",
            "4     0.641858  0.440286  0.384397  0.007248 -0.172761 -0.396514 -0.045816   \n",
            "...        ...       ...       ...       ...       ...       ...       ...   \n",
            "3391  0.001263 -0.053739 -0.086223 -0.316386  0.019711 -0.248028  0.339029   \n",
            "3392 -0.100863 -0.370944  0.241698 -0.030355 -0.287542 -0.235261  0.051112   \n",
            "3393  0.371044 -0.184225  0.645274  0.192262 -0.186283 -0.598438  0.013114   \n",
            "3394  0.492821 -0.352814  0.275192  0.157270 -0.334156 -0.314840  0.385923   \n",
            "3395  0.176367 -0.074738  0.076517 -0.119681 -0.718256  0.234739  0.436135   \n",
            "\n",
            "            8         9         10  ...        55        56        57  \\\n",
            "0                                   ...                                 \n",
            "0     0.308746 -0.082055  0.097792  ...  0.383277 -0.216213 -0.064085   \n",
            "1    -0.121842 -0.121165 -0.131122  ... -0.360806 -0.166503 -0.005197   \n",
            "2    -0.219826  0.133012  0.176054  ... -0.072873  0.019495  0.120134   \n",
            "3    -0.187019  0.226813  0.090419  ...  0.015650  0.107744  0.214450   \n",
            "4    -0.091461 -0.098461 -0.349946  ... -0.219377 -0.103566 -0.097806   \n",
            "...        ...       ...       ...  ...       ...       ...       ...   \n",
            "3391  0.345657  0.021441 -0.152322  ...  0.452250  0.043898 -0.303536   \n",
            "3392 -0.198874 -0.161474  0.327293  ... -0.042114  0.362458 -0.327338   \n",
            "3393  0.187850 -0.176556  0.032825  ...  0.251278 -0.264213 -0.381324   \n",
            "3394 -0.116265  0.753805 -0.380353  ... -0.165607 -0.961766 -0.199239   \n",
            "3395 -0.146291  0.011984 -0.050322  ... -0.055856 -0.444106 -0.131507   \n",
            "\n",
            "            58        59        60        61        62        63        64  \n",
            "0                                                                           \n",
            "0    -0.204484 -0.184131  0.231598  0.384955  0.202194  0.199022 -0.103341  \n",
            "1    -0.170124  0.210439  0.100091 -0.243154  0.075593 -0.208207 -0.102797  \n",
            "2    -0.497160  0.022683 -0.036818  0.207829  0.035628 -0.145883  0.064034  \n",
            "3    -0.243923  0.270832 -0.209652  0.308236  0.095638 -0.120288 -0.106968  \n",
            "4     0.178262  0.242367  0.095277 -0.022931  0.170062 -0.617972 -0.410293  \n",
            "...        ...       ...       ...       ...       ...       ...       ...  \n",
            "3391 -0.423496 -0.034986 -0.053649  0.317083  0.433691 -0.131410  0.010940  \n",
            "3392 -0.069812  0.148060 -0.140282  0.370455  0.507218 -0.353645  0.301798  \n",
            "3393 -0.063013 -0.052293  0.602377  0.145587  0.096426 -0.201407 -0.005251  \n",
            "3394  0.072897  0.142964  0.160078 -0.100865  0.491187  0.024503  0.020933  \n",
            "3395  0.142608  0.387971  0.413358 -0.462996  0.572801 -0.113606  0.103567  \n",
            "\n",
            "[3396 rows x 64 columns]\n",
            "29472\n",
            "7368\n",
            "0\n",
            "29472\n",
            "7368\n",
            "1\n",
            "29472\n",
            "7368\n",
            "2\n",
            "29472\n",
            "7368\n",
            "3\n",
            "29472\n",
            "7368\n",
            "4\n",
            "5-CV\n",
            "ROC fold 0 (AUC=0.8739)\n",
            "ROC fold 1 (AUC=0.8816)\n",
            "ROC fold 2 (AUC=0.8734)\n",
            "ROC fold 3 (AUC=0.8752)\n",
            "ROC fold 4 (AUC=0.8823)\n",
            "Mean ROC (AUC=0.8773)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Graphtransformer"
      ],
      "metadata": {
        "id": "x26kc0DML1Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-geometric\n",
        "!pip install rdkit-pypi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8ZKXDNE0CEpl",
        "outputId": "6a6745b3-74a4-4fac-c0c7-f29841cd6868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (11.0.0)\n",
            "Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import TransformerConv, global_mean_pool\n",
        "from torch_geometric.loader import DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "file_path = '/content/drugs_with_smiles.csv'\n",
        "drug_data = pd.read_csv(file_path)\n",
        "drug_data = drug_data[['drugName', 'SMILES']].dropna(subset=['SMILES'])\n",
        "drug_data['SMILES'] = drug_data['SMILES'].astype(str)\n",
        "\n",
        "\n",
        "def atom_features(atom):\n",
        "    return np.array([\n",
        "        atom.GetAtomicNum(),\n",
        "        atom.GetDegree(),\n",
        "        atom.GetFormalCharge(),\n",
        "        atom.GetIsAromatic()\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    atom_features_list = [atom_features(atom) for atom in mol.GetAtoms()]\n",
        "    x = torch.tensor(atom_features_list, dtype=torch.float)\n",
        "\n",
        "    edges = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edges.append([i, j])\n",
        "        edges.append([j, i])\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index)\n",
        "\n",
        "\n",
        "def extract_features(smiles):\n",
        "    if isinstance(smiles, str):\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "\n",
        "        features = {\n",
        "            \"molecular_weight\": Descriptors.MolWt(mol),\n",
        "            \"logP\": Descriptors.MolLogP(mol),\n",
        "            \"num_rotatable_bonds\": Descriptors.NumRotatableBonds(mol),\n",
        "            \"tpsa\": Descriptors.TPSA(mol),\n",
        "            \"num_heavy_atoms\": Descriptors.HeavyAtomCount(mol),\n",
        "            \"num_atoms\": mol.GetNumAtoms(),\n",
        "        }\n",
        "        return list(features.values())\n",
        "    return None\n",
        "\n",
        "\n",
        "drug_graphs = []\n",
        "drug_names = []\n",
        "drug_features = []\n",
        "for index, row in drug_data.iterrows():\n",
        "    graph = smiles_to_graph(row['SMILES'])\n",
        "    features = extract_features(row['SMILES'])\n",
        "    if graph and features:\n",
        "        drug_graphs.append(graph)\n",
        "        drug_features.append(features)\n",
        "        drug_names.append(row['drugName'])\n",
        "\n",
        "\n",
        "class GraphTransformer(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GraphTransformer, self).__init__()\n",
        "        self.conv1 = TransformerConv(input_dim, hidden_dim, heads=8, dropout=0.1)\n",
        "        self.conv2 = TransformerConv(hidden_dim * 8, hidden_dim, heads=8, dropout=0.1)\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim * 8, hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = global_mean_pool(x, data.batch)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(drug_graphs, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "model = GraphTransformer(input_dim=drug_graphs[0].x.shape[1], hidden_dim=128, output_dim=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def contrastive_loss(embedding1, embedding2, label, margin=1.0):\n",
        "    distance = F.pairwise_distance(embedding1, embedding2, p=2)\n",
        "    loss_pos = label * torch.pow(distance, 2)\n",
        "    loss_neg = (1 - label) * torch.pow(torch.clamp(margin - distance, min=0.0), 2)\n",
        "    loss = torch.mean(loss_pos + loss_neg)\n",
        "    return loss\n",
        "\n",
        "def train_model_with_contrastive_loss(model, loader, epochs=10, margin=1.0):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_embeddings = model(batch)\n",
        "\n",
        "\n",
        "            num_samples = batch_embeddings.shape[0]\n",
        "            pairs = []\n",
        "            labels = []\n",
        "\n",
        "\n",
        "            for i in range(num_samples):\n",
        "                for j in range(i + 1, num_samples):\n",
        "                    pairs.append((batch_embeddings[i], batch_embeddings[j]))\n",
        "                    if random.random() > 0.5:\n",
        "                        labels.append(1)\n",
        "                    else:\n",
        "                        labels.append(0)\n",
        "\n",
        "\n",
        "            pair_embeddings_1 = torch.stack([pair[0] for pair in pairs])\n",
        "            pair_embeddings_2 = torch.stack([pair[1] for pair in pairs])\n",
        "            labels_tensor = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "            loss = contrastive_loss(pair_embeddings_1, pair_embeddings_2, labels_tensor, margin=margin)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(loader)}')\n",
        "\n",
        "\n",
        "train_model_with_contrastive_loss(model, train_loader, epochs=10, margin=1.0)\n",
        "\n",
        "\n",
        "def similarity(A, B):\n",
        "    numerator = torch.dot(A, B)\n",
        "    denominator = A.norm()**2 + B.norm()**2 - numerator\n",
        "    return numerator / denominator if denominator != 0 else 0\n",
        "\n",
        "def compute_similarity_matrix(graphs, model, features):\n",
        "    model.eval()\n",
        "    drug_embeddings = []\n",
        "    for graph in graphs:\n",
        "        with torch.no_grad():\n",
        "            batch = Batch.from_data_list([graph])\n",
        "            embeddings = model(batch)\n",
        "            drug_embeddings.append(embeddings)\n",
        "\n",
        "\n",
        "    drug_embeddings = torch.stack(drug_embeddings).squeeze().numpy()\n",
        "    features_array = np.array(features)\n",
        "\n",
        "\n",
        "    combined_features = np.concatenate([drug_embeddings, features_array], axis=1)\n",
        "\n",
        "\n",
        "    num_drugs = combined_features.shape[0]\n",
        "    similarity_matrix = np.zeros((num_drugs, num_drugs))\n",
        "\n",
        "    for i in range(num_drugs):\n",
        "        for j in range(num_drugs):\n",
        "            similarity_matrix[i][j] = similarity(\n",
        "                torch.tensor(combined_features[i]),\n",
        "                torch.tensor(combined_features[j])\n",
        "            )\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "similarity_matrix = compute_similarity_matrix(drug_graphs, model, drug_features)\n",
        "\n",
        "\n",
        "similarity_df = pd.DataFrame(similarity_matrix, index=drug_names, columns=drug_names)\n",
        "similarity_df.to_csv('drug_similarity_matrix.csv', index=True)\n",
        "\n",
        "\n",
        "print(similarity_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "collapsed": true,
        "id": "YpVJ9kTPBWNb",
        "outputId": "5ab02d8f-b68d-4c2e-dc22-216e8608ab27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drugs_with_smiles.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c0c774db4d57>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drugs_with_smiles.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdrug_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdrug_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrug_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'drugName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SMILES'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SMILES'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdrug_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SMILES'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrug_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SMILES'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drugs_with_smiles.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "final prediction"
      ],
      "metadata": {
        "id": "CB2RspHbBRbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import scipy.sparse as sp\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Partition function\n",
        "def partition(ls, size):\n",
        "    return [ls[i:i + size] for i in range(0, len(ls), size)]\n",
        "\n",
        "# Generate Negative Samples\n",
        "def NegativeGenerate(DrugDisease, AllDrug, AllDisease):\n",
        "    NegativeSample = []\n",
        "    counterN = 0\n",
        "    while counterN < len(DrugDisease):\n",
        "        counterR = random.randint(0, len(AllDrug) - 1)\n",
        "        counterD = random.randint(0, len(AllDisease) - 1)\n",
        "        DiseaseAndRnaPair = [AllDrug[counterR], AllDisease[counterD]]\n",
        "        if DiseaseAndRnaPair in DrugDisease or DiseaseAndRnaPair in NegativeSample:\n",
        "            continue\n",
        "        NegativeSample.append(DiseaseAndRnaPair)\n",
        "        counterN += 1\n",
        "    return NegativeSample\n",
        "\n",
        "def make_prediction(model, drug_name, Attribute, Embedding, drug_name_to_id):\n",
        "    \"\"\"\n",
        "    Make predictions for a given drug by its name and list of all diseases.\n",
        "    Returns the top 10 most likely interactions.\n",
        "\n",
        "    Args:\n",
        "        model: Trained RandomForestClassifier\n",
        "        drug_name: The name of the drug to predict interactions for\n",
        "        Attribute: DataFrame containing attribute information for drugs/diseases\n",
        "        Embedding: DataFrame containing embedding information for drugs/diseases\n",
        "        drug_name_to_id: A dictionary mapping drug names to their corresponding IDs\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with the top 10 predicted disease-drug interactions\n",
        "    \"\"\"\n",
        "    # Ensure the drug name is valid\n",
        "    if drug_name not in drug_name_to_id:\n",
        "        raise ValueError(f\"Drug name {drug_name} not found in the drug name mapping.\")\n",
        "\n",
        "    # Get the drug ID from the drug name\n",
        "    drug_id = drug_name_to_id[drug_name]\n",
        "\n",
        "    if drug_id not in Attribute.index or drug_id not in Embedding.index:\n",
        "        raise ValueError(f\"Drug ID {drug_id} not found in Attribute or Embedding datasets.\")\n",
        "\n",
        "    # Prepare feature set for the given drug\n",
        "    drug_feature = pd.concat([\n",
        "        Attribute.loc[[drug_id]].reset_index(drop=True),\n",
        "        Embedding.loc[[drug_id]].reset_index(drop=True)\n",
        "    ], axis=1)\n",
        "\n",
        "    # Prepare features for all diseases\n",
        "    disease_ids = Attribute.index.tolist()  # Assuming all disease IDs are available in the Attribute DataFrame\n",
        "    disease_feature = pd.concat([\n",
        "        Attribute.loc[disease_ids].reset_index(drop=True),\n",
        "        Embedding.loc[disease_ids].reset_index(drop=True)\n",
        "    ], axis=1)\n",
        "\n",
        "    # Combine drug and disease features for prediction\n",
        "    features = pd.concat([drug_feature] * len(disease_ids), axis=0, ignore_index=True)\n",
        "    disease_ids_expanded = pd.DataFrame({'disease_id': disease_ids})\n",
        "    features = pd.concat([features, disease_feature], axis=1)\n",
        "\n",
        "    # Make predictions\n",
        "    probabilities = model.predict_proba(features)[:, 1]\n",
        "\n",
        "    # Create a DataFrame to store the results\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'disease_id': disease_ids,\n",
        "        'interaction_probability': probabilities\n",
        "    })\n",
        "\n",
        "    # Sort by interaction probability (descending) and return top 10\n",
        "    top_10_predictions = predictions_df.sort_values(by='interaction_probability', ascending=False).head(10)\n",
        "    return top_10_predictions\n",
        "\n",
        "\n",
        "# Main function\n",
        "def main(options):\n",
        "    if options.dataset == 1:\n",
        "        dataset = 'B-Dataset'\n",
        "    else:\n",
        "        dataset = 'F-Dataset'\n",
        "    print('fold', options.fold_num)\n",
        "    print(dataset)\n",
        "\n",
        "    # Load data\n",
        "    DrDiNum18416 = pd.read_csv('/content/DrDiNum.csv', header=None)\n",
        "    DrPrNum3243 = pd.read_csv('/content/DrPrNum.csv', header=None)\n",
        "    DiPrNum71840 = pd.read_csv('/content/DiPrNum.csv', header=None)\n",
        "\n",
        "    # Shuffle and partition the data\n",
        "    RandomList = random.sample(range(0, len(DrDiNum18416)), len(DrDiNum18416))\n",
        "    print('len(RandomList)', len(RandomList))\n",
        "    NewRandomList = partition(RandomList, math.ceil(len(RandomList) / options.fold_num))\n",
        "    print('len(NewRandomList[0])', len(NewRandomList[0]))\n",
        "    NewRandomList = pd.DataFrame(NewRandomList).fillna(0).astype(int)\n",
        "    NewRandomList.to_csv('/content/NewRandomList.csv', header=None, index=False)\n",
        "    del NewRandomList, RandomList\n",
        "\n",
        "    # Cross-validation split\n",
        "    Nindex = pd.read_csv('/content/NewRandomList.csv', header=None)\n",
        "    for i in range(len(Nindex)):\n",
        "        kk = [j for j in range(options.fold_num) if j != i]\n",
        "        index = np.hstack([np.array(Nindex)[k] for k in kk])\n",
        "\n",
        "        # Ensure that indices exist before trying to access them\n",
        "        valid_index = [idx for idx in index if idx in DrDiNum18416.index]\n",
        "\n",
        "        DTIs_train = pd.DataFrame(np.array(DrDiNum18416)[valid_index])\n",
        "        DTIs_train.to_csv(f'/content/DrDiIs_train{i}.csv', header=None, index=False)\n",
        "\n",
        "        test_indices = np.array(Nindex)[i].tolist()\n",
        "        valid_test_indices = [idx for idx in test_indices if idx in DrDiNum18416.index]\n",
        "\n",
        "        DTIs_test = pd.DataFrame(np.array(DrDiNum18416)[valid_test_indices])\n",
        "        DTIs_test.to_csv(f'/content/DrDiIs_test{i}.csv', header=None, index=False)\n",
        "        print(i)\n",
        "    del Nindex, index, DTIs_train, DTIs_test\n",
        "\n",
        "    # Combine data\n",
        "    DTIs_train = pd.concat([DrDiNum18416, DrPrNum3243, DiPrNum71840]).sample(frac=1.0)\n",
        "    DTIs_train.to_csv('/content/AllDrDiIs_train.txt', sep='\\t', header=None, index=False)\n",
        "\n",
        "    # Generate negative samples\n",
        "    Dr = pd.read_csv('/content/drugName.csv', header=0, names=['id', 'name'])\n",
        "    Pr = pd.read_csv('/content/diseaseName.csv', header=0, names=['id', 'name'])\n",
        "    NegativeSample = NegativeGenerate(DrDiNum18416.values.tolist(), Dr['id'].values.tolist(), Pr['id'].values.tolist())\n",
        "    NegativeSample = pd.DataFrame(NegativeSample)\n",
        "    NegativeSample.to_csv('/content/NegativeSample.csv', header=None, index=False)\n",
        "    drug_name_to_id = dict(zip(Dr['name'], Dr['id']))\n",
        "    print(drug_name_to_id)\n",
        "\n",
        "    # Load additional data\n",
        "    Negative = pd.read_csv('/content/NegativeSample.csv', header=None)\n",
        "    Nindex = pd.read_csv('/content/NewRandomList.csv', header=None)\n",
        "    Attribute = pd.read_csv('/content/AllNodeAttribute.csv', header=None, index_col=0).iloc[:, 1:]\n",
        "    Embedding = pd.read_csv('/content/AllEmbedding_DeepWalk.txt', sep=' ', header=None, skiprows=1)\n",
        "    Embedding = Embedding.sort_values(0, ascending=True).set_index([0])\n",
        "    print(Embedding)\n",
        "\n",
        "    # Prepare training and testing data for each fold\n",
        "    for i in range(options.fold_num):\n",
        "        train_data = pd.read_csv(f'/content/DrDiIs_train{i}.csv', header=None)\n",
        "        train_data[2] = 1\n",
        "        kk = [j for j in range(options.fold_num) if j != i]\n",
        "        index = np.hstack([np.array(Nindex)[k] for k in kk])\n",
        "        result = pd.concat([train_data, pd.DataFrame(np.array(Negative)[index])])\n",
        "        labels_train = result[2]\n",
        "        data_train_feature = pd.concat([\n",
        "            pd.concat([Attribute.loc[result[0].values.tolist()], Embedding.loc[result[0].values.tolist()]], axis=1).reset_index(drop=True),\n",
        "            pd.concat([Attribute.loc[result[1].values.tolist()], Embedding.loc[result[1].values.tolist()]], axis=1).reset_index(drop=True)\n",
        "        ], axis=1)\n",
        "\n",
        "        # Replace NaN values in labels_train with 0\n",
        "        labels_train.fillna(0, inplace=True)\n",
        "\n",
        "        globals()[f'data_train{i}'] = data_train_feature.values.tolist()\n",
        "        globals()[f'labels_train{i}'] = labels_train\n",
        "        print(len(labels_train))\n",
        "        del result, data_train_feature\n",
        "\n",
        "        test_data = pd.read_csv(f'/content/DrDiIs_test{i}.csv', header=None)\n",
        "        test_data[2] = 1\n",
        "        result = pd.concat([test_data, pd.DataFrame(np.array(Negative)[np.array(Nindex)[i]])])\n",
        "\n",
        "        # Replace NaN values in labels_test with 0\n",
        "        labels_test = result[2]\n",
        "        labels_test.fillna(0, inplace=True)\n",
        "\n",
        "        data_test_feature = pd.concat([\n",
        "            pd.concat([Attribute.loc[result[0].values.tolist()], Embedding.loc[result[0].values.tolist()]], axis=1).reset_index(drop=True),\n",
        "            pd.concat([Attribute.loc[result[1].values.tolist()], Embedding.loc[result[1].values.tolist()]], axis=1).reset_index(drop=True)\n",
        "        ], axis=1)\n",
        "        globals()[f'data_test{i}'] = data_test_feature.values.tolist()\n",
        "        globals()[f'labels_test{i}'] = labels_test\n",
        "        print(len(labels_test))\n",
        "        del test_data, labels_test, result, data_test_feature\n",
        "        print(i)\n",
        "\n",
        "    # Aggregate data for training and testing\n",
        "    data_train = [globals()[f'data_train{i}'] for i in range(options.fold_num)]\n",
        "    data_test = [globals()[f'data_test{i}'] for i in range(options.fold_num)]\n",
        "    labels_train = [globals()[f'labels_train{i}'] for i in range(options.fold_num)]\n",
        "    labels_test = [globals()[f'labels_test{i}'] for i in range(options.fold_num)]\n",
        "\n",
        "    # Cross-validation and ROC AUC computation\n",
        "    print(f\"{options.fold_num}-CV\")\n",
        "    tprs = []\n",
        "    aucs = []\n",
        "    mean_fpr = np.linspace(0, 1, 1000)\n",
        "    AllResult = []\n",
        "\n",
        "    for i in range(1):\n",
        "        X_train, X_test = data_train[i], data_test[i]\n",
        "        Y_train, Y_test = np.array(labels_train[i]), np.array(labels_test[i])\n",
        "\n",
        "        # Replace NaN values with 0 in Y_train and Y_test\n",
        "        Y_train[np.isnan(Y_train)] = 0\n",
        "        Y_test[np.isnan(Y_test)] = 0\n",
        "\n",
        "        best_RandomF = RandomForestClassifier(n_estimators=options.tree_number)\n",
        "        best_RandomF.fit(X_train, Y_train)\n",
        "\n",
        "        # Predict probabilities and handle NaN values in y_score_RandomF\n",
        "        y_score_RandomF = best_RandomF.predict_proba(X_test)\n",
        "        y_score_RandomF[np.isnan(y_score_RandomF)] = 0\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(Y_test, y_score_RandomF[:, 1])\n",
        "        tprs.append(np.interp(mean_fpr, fpr, tpr))  # Changed here to np.interp\n",
        "        tprs[-1][0] = 0.0\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        aucs.append(roc_auc)\n",
        "        print('ROC fold %d (AUC=%0.4f)' % (i, roc_auc))\n",
        "\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    print('Mean ROC (AUC=%0.4f)' % mean_auc)\n",
        "\n",
        "    print(Dr['id'].values.tolist())\n",
        "\n",
        "    drug_name = 'benazepril'  # Change to a valid drug ID from your dataset\n",
        "\n",
        "    predictions = make_prediction(best_RandomF, drug_name, Attribute, Embedding, drug_name_to_id)\n",
        "    print(\"\\nTop 10 predicted interactions for drug:\", drug_name)\n",
        "    print(predictions)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import optparse\n",
        "    import sys\n",
        "\n",
        "    # Check if running in a Jupyter notebook or parse command-line arguments\n",
        "    if 'ipykernel' in sys.modules or any('jupyter' in arg for arg in sys.argv):\n",
        "        class Options:\n",
        "            def __init__(self, dataset, fold_num, tree_number):\n",
        "                self.dataset = dataset\n",
        "                self.fold_num = fold_num\n",
        "                self.tree_number = tree_number\n",
        "\n",
        "        options = Options(dataset=1, fold_num=5, tree_number=100)  # Example options, customize as needed\n",
        "    else:\n",
        "        optparser = optparse.OptionParser()\n",
        "        optparser.add_option('-d', '--dataset', dest='dataset', type='int', default=1)\n",
        "        optparser.add_option('-f', '--fold_num', dest='fold_num', type='int', default=5)\n",
        "        optparser.add_option('-t', '--tree_number', dest='tree_number', type='int', default=100)\n",
        "        (options, args) = optparser.parse_args()\n",
        "\n",
        "    main(options)\n"
      ],
      "metadata": {
        "id": "JOyVLpa2i6Mw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "978e02c8-8b40-423c-8b5f-062a012ba90b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 5\n",
            "B-Dataset\n",
            "len(RandomList) 18416\n",
            "len(NewRandomList[0]) 3684\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "{'vinorelbine': 1, 'carvedilol': 2, 'benazepril': 3, 'leflunomide': 4, 'lamotrigine': 5, 'nefazodone': 6, 'trandolapril': 7, 'clopidogrel': 8, 'gemcitabine': 9, 'troglitazone': 10, 'pioglitazone': 11, 'zafirlukast': 12, 'mycophenolate mofetil': 13, 'fluvastatin': 14, 'meloxicam': 15, 'docetaxel': 16, 'olanzapine': 17, 'candesartan': 18, 'telmisartan': 19, 'bosentan': 20, 'cerivastatin': 21, 'rosiglitazone': 22, 'fexofenadine': 23, 'montelukast': 24, 'alitretinoin': 25, 'rofecoxib': 26, 'desloratadine': 27, 'gefitinib': 28, 'sorafenib': 29, 'sunitinib': 30, 'celecoxib': 31, 'sildenafil citrate': 32, 'rosuvastatin calcium': 33, 'valsartan': 34, 'imatinib mesylate': 35, 'sitagliptin phosphate': 36, 'atorvastatin calcium': 37, 'erlotinib hydrochloride': 38, 'quetiapine fumarate': 39, 'ezetimibe': 40, 'dasatinib': 41, 'venlafaxine hydrochloride': 42, 'acebutolol': 43, 'acetaminophen': 44, 'acetazolamide': 45, 'acetylcholine': 46, 'adenosine': 47, 'allopurinol': 48, 'amantadine': 49, 'amiloride': 50, 'amiodarone': 51, 'amitriptyline': 52, 'amphetamine': 53, 'amsacrine': 54, 'antipyrine': 55, 'aspirin': 56, 'betamethasone': 57, 'bezafibrate': 58, 'biperiden': 59, 'bromocriptine': 60, 'bumetanide': 61, 'buprenorphine': 62, 'buspirone': 63, 'caffeine': 64, 'carbamazepine': 65, 'cefazolin': 66, 'ceftriaxone': 67, 'chloramphenicol': 68, 'chloroquine': 69, 'chlorpheniramine': 70, 'chlorpromazine': 71, 'chlorpropamide': 72, 'choline': 73, 'cimetidine': 74, 'ciprofloxacin': 75, 'clofazimine': 76, 'clomiphene': 77, 'clomipramine': 78, 'clonidine': 79, 'clotrimazole': 80, 'clozapine': 81, 'cocaine': 82, 'colchicine': 83, 'cytarabine': 84, 'daunorubicin': 85, 'desipramine': 86, 'dexamethasone': 87, 'dextromethorphan': 88, 'diazepam': 89, 'diclofenac': 90, 'diethylstilbestrol': 91, 'diflunisal': 92, 'digitoxin': 93, 'digoxin': 94, 'dihydroergotamine': 95, 'diltiazem': 96, 'diphenhydramine': 97, 'disopyramide': 98, 'domperidone': 99, 'dopamine': 100, 'doxepin': 101, 'doxorubicin': 102, 'doxycycline': 103, 'enalapril': 104, 'epinephrine': 105, 'ergotamine': 106, 'erythromycin': 107, 'estradiol': 108, 'estramustine': 109, 'estrogens, conjugated (usp)': 110, 'estrone': 111, 'ethacrynic acid': 112, 'ethinyl estradiol': 113, 'etoposide': 114, 'fentanyl': 115, 'fluorouracil': 116, 'fluoxetine': 117, 'flupenthixol': 118, 'fluphenazine': 119, 'flurazepam': 120, 'flurbiprofen': 121, 'folic acid': 122, 'furosemide': 123, 'glyburide': 124, 'glycerol': 125, 'haloperidol': 126, 'hydrocortisone': 127, 'ibuprofen': 128, 'imipramine': 129, 'indomethacin': 130, 'ivermectin': 131, 'ketamine': 132, 'ketoconazole': 133, 'ketoprofen': 134, 'levodopa': 135, 'lidocaine': 136, 'loperamide': 137, 'lovastatin': 138, 'maprotiline': 139, 'mebendazole': 140, 'melatonin': 141, 'memantine': 142, 'meprobamate': 143, 'methadone': 144, 'methamphetamine': 145, 'methionine': 146, 'methotrexate': 147, 'methyldopa': 148, 'methylprednisolone': 149, 'metoprolol': 150, 'metyrapone': 151, 'miconazole': 152, 'midazolam': 153, 'midodrine': 154, 'mitoxantrone': 155, 'morphine': 156, 'ipratropium': 157, 'naloxone': 158, 'naltrexone': 159, 'naproxen': 160, 'neostigmine': 161, 'niacin': 162, 'nicardipine': 163, 'nicotine': 164, 'nifedipine': 165, 'nitrazepam': 166, 'nitrendipine': 167, 'norepinephrine': 168, 'norfloxacin': 169, 'ouabain': 170, 'oxprenolol': 171, 'pancuronium': 172, 'phenacetin': 173, 'phenformin': 174, 'phenobarbital': 175, 'phenoxybenzamine': 176, 'phenylbutazone': 177, 'phenytoin': 178, 'pimozide': 179, 'piroxicam': 180, 'prazosin': 181, 'prednisolone': 182, 'prednisone': 183, 'probenecid': 184, 'procainamide': 185, 'progesterone': 186, 'promethazine': 187, 'propafenone': 188, 'propranolol': 189, 'protriptyline': 190, 'quercetin': 191, 'quinacrine': 192, 'quinidine': 193, 'quinine': 194, 'ranitidine': 195, 'reserpine': 196, 'ribavirin': 197, 'riboflavin': 198, 'rifampin': 199, 'sulfasalazine': 200, 'selegiline': 201, 'spironolactone': 202, 'streptozocin': 203, 'sulfinpyrazone': 204, 'sulindac': 205, 'tacrine': 206, 'tamoxifen': 207, 'testosterone': 208, 'tetracycline': 209, 'dronabinol': 210, 'theophylline': 211, 'thiothixene': 212, 'thyroxine': 213, 'timolol': 214, 'tolbutamide': 215, 'tolmetin': 216, 'trazodone': 217, 'tretinoin': 218, 'trifluoperazine': 219, 'trimethoprim': 220, 'trimipramine': 221, 'ursodeoxycholic acid': 222, 'valproic acid': 223, 'verapamil': 224, 'vinblastine': 225, 'vincristine': 226, 'vitamin b 12': 227, 'zidovudine': 228, 'dinoprostone': 229, 'ofloxacin': 230, 'gemfibrozil': 231, 'epirubicin': 232, 'idarubicin': 233, 'citalopram': 234, 'mifepristone': 235, 'felodipine': 236, 'nisoldipine': 237, 'famotidine': 238, 'alfentanil': 239, 'bepridil': 240, 'albendazole': 241, 'mefloquine': 242, 'astemizole': 243, 'terfenadine': 244, 'fluvoxamine': 245, 'arachidonic acid': 246, 'pravastatin': 247, 'paclitaxel': 248, 'ramipril': 249, 'clarithromycin': 250, 'doxazosin': 251, 'etodolac': 252, 'toremifene': 253, 'loratadine': 254, 'cladribine': 255, 'paroxetine': 256, 'itraconazole': 257, 'sumatriptan': 258, 'risperidone': 259, 'riluzole': 260, 'losartan': 261, 'simvastatin': 262, 'genistein': 263, 'sirolimus': 264, 'salicylic acid': 265, 'sertraline': 266, 'mibefradil': 267, 'perindopril': 268}\n",
            "            1         2         3         4         5         6         7   \\\n",
            "0                                                                            \n",
            "0     0.036320  0.114171 -0.111191 -0.306078 -0.150277 -0.242947  0.091169   \n",
            "1     0.305414  0.095406  0.015667 -0.047628 -0.128059 -0.174679  0.038152   \n",
            "2    -0.256181  0.181465  0.079849 -0.457883 -0.287999 -0.031660  0.076192   \n",
            "3     0.001911  0.204960  0.024538 -0.404577 -0.621718 -0.363406  0.318680   \n",
            "4     0.641858  0.440286  0.384397  0.007248 -0.172761 -0.396514 -0.045816   \n",
            "...        ...       ...       ...       ...       ...       ...       ...   \n",
            "3391  0.001263 -0.053739 -0.086223 -0.316386  0.019711 -0.248028  0.339029   \n",
            "3392 -0.100863 -0.370944  0.241698 -0.030355 -0.287542 -0.235261  0.051112   \n",
            "3393  0.371044 -0.184225  0.645274  0.192262 -0.186283 -0.598438  0.013114   \n",
            "3394  0.492821 -0.352814  0.275192  0.157270 -0.334156 -0.314840  0.385923   \n",
            "3395  0.176367 -0.074738  0.076517 -0.119681 -0.718256  0.234739  0.436135   \n",
            "\n",
            "            8         9         10  ...        55        56        57  \\\n",
            "0                                   ...                                 \n",
            "0     0.308746 -0.082055  0.097792  ...  0.383277 -0.216213 -0.064085   \n",
            "1    -0.121842 -0.121165 -0.131122  ... -0.360806 -0.166503 -0.005197   \n",
            "2    -0.219826  0.133012  0.176054  ... -0.072873  0.019495  0.120134   \n",
            "3    -0.187019  0.226813  0.090419  ...  0.015650  0.107744  0.214450   \n",
            "4    -0.091461 -0.098461 -0.349946  ... -0.219377 -0.103566 -0.097806   \n",
            "...        ...       ...       ...  ...       ...       ...       ...   \n",
            "3391  0.345657  0.021441 -0.152322  ...  0.452250  0.043898 -0.303536   \n",
            "3392 -0.198874 -0.161474  0.327293  ... -0.042114  0.362458 -0.327338   \n",
            "3393  0.187850 -0.176556  0.032825  ...  0.251278 -0.264213 -0.381324   \n",
            "3394 -0.116265  0.753805 -0.380353  ... -0.165607 -0.961766 -0.199239   \n",
            "3395 -0.146291  0.011984 -0.050322  ... -0.055856 -0.444106 -0.131507   \n",
            "\n",
            "            58        59        60        61        62        63        64  \n",
            "0                                                                           \n",
            "0    -0.204484 -0.184131  0.231598  0.384955  0.202194  0.199022 -0.103341  \n",
            "1    -0.170124  0.210439  0.100091 -0.243154  0.075593 -0.208207 -0.102797  \n",
            "2    -0.497160  0.022683 -0.036818  0.207829  0.035628 -0.145883  0.064034  \n",
            "3    -0.243923  0.270832 -0.209652  0.308236  0.095638 -0.120288 -0.106968  \n",
            "4     0.178262  0.242367  0.095277 -0.022931  0.170062 -0.617972 -0.410293  \n",
            "...        ...       ...       ...       ...       ...       ...       ...  \n",
            "3391 -0.423496 -0.034986 -0.053649  0.317083  0.433691 -0.131410  0.010940  \n",
            "3392 -0.069812  0.148060 -0.140282  0.370455  0.507218 -0.353645  0.301798  \n",
            "3393 -0.063013 -0.052293  0.602377  0.145587  0.096426 -0.201407 -0.005251  \n",
            "3394  0.072897  0.142964  0.160078 -0.100865  0.491187  0.024503  0.020933  \n",
            "3395  0.142608  0.387971  0.413358 -0.462996  0.572801 -0.113606  0.103567  \n",
            "\n",
            "[3396 rows x 64 columns]\n",
            "29472\n",
            "7368\n",
            "0\n",
            "29472\n",
            "7368\n",
            "1\n",
            "29472\n",
            "7368\n",
            "2\n",
            "29472\n",
            "7368\n",
            "3\n",
            "29472\n",
            "7368\n",
            "4\n",
            "5-CV\n",
            "ROC fold 0 (AUC=0.8769)\n",
            "Mean ROC (AUC=0.8769)\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268]\n",
            "\n",
            "Top 10 predicted interactions for drug: benazepril\n",
            "     disease_id  interaction_probability\n",
            "412        1168                     0.96\n",
            "278        1034                     0.95\n",
            "316        1072                     0.93\n",
            "359        1115                     0.93\n",
            "403        1159                     0.92\n",
            "405        1161                     0.91\n",
            "421        1177                     0.90\n",
            "398        1154                     0.89\n",
            "416        1172                     0.89\n",
            "422        1178                     0.88\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNn9d93WVWaOLgLH2qvk130",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}